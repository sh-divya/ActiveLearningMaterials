# more epochs, larger batch size, explore faenet: larger model & skip-co & mlp_rij
job:
  mem: 32GB
  cpus: 4
  gres: gpu:1
  partition: long

default:
  optim: 
    epochs: 20

runs:
  - config: physmlp-mp20
    wandb_note: "PhysMLP baseline"
    model:
      num_layers: 2
      hidden_channels: 512
      sg_emb_size: 128
      lat_hidden_channels: 64
      lat_num_layers: 1
      comp_num_layers: 1
      comp_hidden_channels: 64
      comp_phys_embeds:
        use: True
        z_emb_size: 32
        period_emb_size: 16
        group_emb_size: 16
        properties_proj_size: 32
        n_elements: 90
    optim:
      batch_size: 32
      lr: 0.001
      es_patience: 3